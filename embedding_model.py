## CS 482/682: Machine Learning: Deep Learning Final Project
# ## Malware Prediction
# 
# This is the impletation of the second model, using embeddings and frequency encoding with neural networks, training on entire dataset.

# ## Data Description
# 
# Recall that our training data shape is (8920000, 83), and each column represents a feature of a machine, each row represents a specific machine.

# ### Define different types of data column. 
# 
# - Normal: This kind of features are categorical features, often with a lot of unique values. For example, `AvSigVersion` it may be some frequently updated software on a computer, which has 8531 unique values. `OSVersion` represents the OS version of that computer, and has 469 unique values. When dealing with this kind of feature, we can simply use them as the input, because it already has a lot of information and categories feature can be used directly, it is not feasible to encode them as one-hot format.
# 
# - Numerical_Encoding: This is kind of feature with numeric values. For example, `SystemVolumeTotalCapacity` is the total amount of storaging space, which is a numerical value. Thus, these features are put as numerical values, to be numerical encoded. 
# 
# - Onehot_Encoding: This kind of features are also categorical features, different from Normal Features, they have less unique values. For example, `IsProtected` is a binary feature, `PrimaryDiskTypeName` has only 4 unique values, that is ['HDD', 'SSD', 'UNKNOWN', 'Unspecified']. Thus, these kind of features we use them as one-hot-encoded. 
# 
# - Drop: This kind of features are already experimented by others that there are often 90% above missing values. Thus, we can directly ignore them and won't affect the model too much.

# In debug mode, only 10000 lines of data are loaded
Debug = False

import pandas as pd, numpy as np, os
import math
from keras import callbacks
from sklearn.metrics import roc_auc_score
from keras.models import Model
from keras.layers import Dense, Input, concatenate, BatchNormalization, Activation, Dropout, Embedding, Reshape
from keras.callbacks import LearningRateScheduler
from keras.optimizers import Adam
from utils import * 
from encode_patterns import * 


dtypes = {}
for x in Onehot_Encoding+Numerical_Encoding+Normal: dtypes[x] = 'category'
dtypes['HasDetections'] = 'int8' # label is numeric


if Debug:
    df_train = pd.read_csv('../input/microsoft-malware-prediction/train.csv', usecols=dtypes.keys(), dtype=dtypes, nrows=10000)
else:
    df_train = pd.read_csv('../input/microsoft-malware-prediction/train.csv', usecols=dtypes.keys(), dtype=dtypes)
    
if 5244810 in df_train.index:
    df_train.loc[5244810,'AvSigVersion'] = '1.273.1144.0'
    df_train['AvSigVersion'].cat.remove_categories('1.2&#x17;3.1144.0',inplace=True)
print ('Loaded',len(df_train),'rows of TRAIN.CSV!')

df_train = df_train.sample(frac=1)
df_train.reset_index(drop=True,inplace=True)

if Debug:
    df_test = pd.read_csv('../input/microsoft-malware-prediction/test.csv', usecols=list(dtypes.keys())[0:-1], dtype=dtypes,nrows=10000)
else:
    df_test = pd.read_csv('../input/microsoft-malware-prediction/test.csv', usecols=list(dtypes.keys())[0:-1], dtype=dtypes)


# ## Define Encoding Functions
# 
# ### Further code review:
# 
#    - factor data: Factorize is a function to change all the character/strings to numbers. For example: ['a', 'a', 'b', 'c', 'a'] => [0, 0, 1, 2, 0]. About pd.factorize [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.factorize.html]. 
#    
#    - reduce_memory: pass
#    
#    - encode_FE_lg: This function deals with each feature, for each feature(column), it computes the occurance of each unique value, take log of it and normalize to [0, 1]. The use of this function is to simply get the frequency of each value among each feature.
#    
#    - encode_CE: This is the implementation of statistical category encoding. It statistically count each unique value from each column, if this value has approx. 0.5 chance to influence `HasDetection`, we can infer that this value is not relevant to prediction, thus we can label it as "irrelevant". By doing this, we can significantlly decrease the unique value in each column, and get rid of those irrelevant values.
#    
#    - encode_CE_test: Test to see if the column is already encoded.


def factor_data(df_train, df_test, col):
    df_comb = pd.concat([df_train[col],df_test[col]],axis=0) 
    df_comb,_ = df_comb.factorize(sort=True) 
    df_comb += 1 
    df_comb = np.where(df_comb==0, df_comb.max()+1, df_comb) 
    df_train[col] = df_comb[:len(df_train)] 
    df_test[col] = df_comb[len(df_train):]
    del df_comb
    mx = max(df_train[col].max(),df_test[col].max())+1 
    return mx
    
def reduce_memory(df,col): 
    mx = df[col].max()
    if mx<256:
            df[col] = df[col].astype('uint8')
    elif mx<65536:
        df[col] = df[col].astype('uint16')
    else:
        df[col] = df[col].astype('uint32')
    
def encode_FE_lg(df,col,verbose=1): 
    ln = 1/df[col].nunique()  
    vc = (df[col].value_counts(dropna=False, normalize=True)+ln).map(math.log).to_dict() 

    nm = col+'_FE_lg'  
    df[nm] = df[col].map(vc) 
    df[nm] -= df[nm].min() 
    df[nm] = df[nm]/df[nm].max()
    df[nm] = df[nm].astype('float32')
    if verbose==1:
        print('FE encoded',col)
    return [nm] 


def encode_CE(df, col, filter, zscore, tar='HasDetections', m=0.5, verbose=1):  
    cv = pd.DataFrame( df[col].value_counts(dropna=False) ).reset_index()
    cv4 = df.groupby(col)[tar].mean().reset_index().rename({tar:'rate',col:'index'},axis=1)
    d1 = set(cv['index'].unique())
    cv = pd.merge(cv,cv4,on='index',how='left')
    if (len( cv[ cv['index'].isna() ])!=0 ):
        cv.loc[ cv['index'].isna(),'rate' ] = df.loc[ df[col].isna(),tar ].mean()
    cv = cv[ cv[col]> (filter * len(df)) ]
    cv['ratec'] = (df[tar].sum() - cv['rate']*cv[col])/(len(df)-cv[col])
    cv['sd'] = zscore * 0.5 / cv[col].map(lambda x: math.sqrt(x))
    cv = cv[ (abs(cv['rate']-m)>=cv['sd']) | (abs(cv['ratec']-1+m)>=cv['sd']) ]
    d2 = set(cv['index'].unique())
    d = list(d1 - d2)
    if (df[col].dtype.name=='category'):
        if (not 0 in df[col].cat.categories):
            df[col].cat.add_categories(0,inplace=True)
        else:
            print('0 ALREADY EXISTS IN',col)
    df.loc[ df[col].isin(d),col ] = 0
    if verbose==1:
        print('CE encoded',col,'-',len(d2),'values. Removed',len(d),'values')
    mx = df[col].nunique() 
    return [mx,d2]

def encode_CE_test(df,col,d):
    if (df[col].dtype.name=='category'):
        if (not 0 in df[col].cat.categories):
            df[col].cat.add_categories(0,inplace=True)
        else:
            print('0 ALREADY EXISTS IN',col)
    df.loc[ ~df[col].isin(d),col ] = 0
    mx = df[col].nunique()
    return [mx,d]


# ## Add Some Important New Features
def makeNew(df,verbose=1,add=0,TS=True,data=0):

    old = df.columns
    
    # FEATURE ENGINEER
    df['AppVersion2'] = df['AppVersion'].apply(lambda x: x.split('.')[1]).astype('category')

    if TS:
        from datetime import datetime, date, timedelta

        # AS timestamp
        datedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy')[()]
        df['DateAS'] = df['AvSigVersion'].map(datedictAS)

        # OS timestamp
        datedictOS = np.load('../input/malware-timestamps-2/OSVersionTimestamps.npy')[()]
        df['DateOS'] = df['Census_OSVersion'].map(datedictOS)

        df['Lag1'] = df['DateAS'] - df['DateOS']
        df['Lag1'] = df['Lag1'].map(lambda x: x.days//7)
        df['Lag1'] = df['Lag1']/52.0
        df['Lag1'] = df['Lag1'].astype('float32')
        df['Lag1'].fillna(0,inplace=True)
        
        if data!=0:
            if data==1:
                df['Lag5'] = datetime(2018,7,26) - df['DateAS'] # TRAIN
            elif data==2:
                df['Lag5'] = datetime(2018,9,27) - df['DateAS'] #PUBLIC TEST
            elif data==3:
                df['Lag5'] = datetime(2018,10,27) - df['DateAS'] #PRIVATE TEST
            df['Lag5'] = df['Lag5'].map(lambda x: x.days//1)
            df.loc[ df['Lag5']<0, 'Lag5' ] = 0
            df['Lag5'] = df['Lag5']/365.0
            df['Lag5'] = df['Lag5'].astype('float32')
            df['Lag5'].fillna(0,inplace=True)

        del df['DateAS'], df['DateOS']
        del datedictAS, datedictOS
    
    for col in Numerical_Encoding:
        nm = col+'_NE'
        df[nm] = df[col].astype('float32')
        df[nm] /= np.std(df[nm])
    new = list(set(df.columns)-set(old))
    ret = []
    for x in new:
        if str(df[x].dtype)=='category': # if cat
            if add==1: Onehot_Encoding.append(x)
        else: 
            ret.append(x)
            df[x].fillna(df[x].mean(),inplace=True)
    if verbose==1:
        print('Engineered',len(new),'new features!')
    return ret


# ## Actually Encode Those Variables
# This is the place to encode each feature column, using the functions above.

FE = []
for col in df_train.columns:
    if col=='HasDetections': continue
    if df_train[col].nunique()>10: 
        FE.append(col) 
        
NUM = makeNew(df_train,verbose=0,add=1,data=1) 
makeNew(df_test,verbose=0,data=2) 
ct = len(NUM)+1; cnew = ct

for x in FE: 
    NUM += encode_FE_lg(df_train,x,verbose=0)
    encode_FE_lg(df_test,x,verbose=0)
    ct += 1
    
# Statistical Category Encoding
inps={}; tt = 0
for col in Onehot_Encoding: 
    factor_data(df_train,df_test,col) 
    d = encode_CE(df_train,col,0.001,1)[1] 
    encode_CE_test(df_test,col,d)
    inps[col] = factor_data(df_train,df_test,col)
    tt += inps[col]
    reduce_memory(df_train,col)
    reduce_memory(df_test,col)
    ct += 1

for x in np.unique(Numerical_Encoding+Normal):
    del df_train[x]
    if x!='AvSigVersion': del df_test[x]


mm = round(df_train.memory_usage(deep=True).sum() / 1024**2)
mm2 = round(df_test.memory_usage(deep=True).sum() / 1024**2)

# # Define Variable Groupings
# All variables in the same group get "squeezed" together and thus the group's dimension is reduced.


# DEFINE NETWORK ARCHITECTURE GROUPINGS
# (1) GEOGRAPHICAL, (2) SOFTWARE/VIRUS, (3) HARDWARE, (4) NAME/MODEL

groups = [  ['CountryIdentifier','CityIdentifier','OrganizationIdentifier','GeoNameIdentifier',
             'LocaleEnglishNameIdentifier','Census_OSInstallLanguageIdentifier','Census_OSUILocaleIdentifier',
            'Wdft_RegionIdentifier'],
            ['DefaultBrowsersIdentifier', 'AVProductStatesIdentifier', 'AVProductsInstalled', 'AVProductsEnabled',
             'IsProtected', 'SMode', 'IeVerIdentifier', 'SmartScreen', 'Firewall','Census_IsSecureBootEnabled',
            'Census_IsWIMBootEnabled','Wdft_IsGamer','Census_OSWUAutoUpdateOptionsName','Census_GenuineStateName',
            'AppVersion2'],
            ['Processor','Census_MDC2FormFactor','Census_DeviceFamily','Census_ProcessorCoreCount','Census_ProcessorClass',
            'Census_PrimaryDiskTypeName','Census_HasOpticalDiskDrive','Census_TotalPhysicalRAM','Census_ChassisTypeName',
            'Census_InternalPrimaryDiagonalDisplaySizeInInches', 'Census_InternalPrimaryDisplayResolutionHorizontal',
            'Census_InternalPrimaryDisplayResolutionVertical', 'Census_PowerPlatformRoleName', 'Census_InternalBatteryType',
            'Census_InternalBatteryNumberOfCharges','Census_IsTouchEnabled','Census_IsPenCapable',
             'Census_IsAlwaysOnAlwaysConnectedCapable'],
            ['Census_OEMNameIdentifier', 'Census_OEMModelIdentifier', 'Census_ProcessorManufacturerIdentifier',
            'Census_ProcessorModelIdentifier','Census_FirmwareManufacturerIdentifier', 'Census_FirmwareVersionIdentifier']
         ]

df_train_Y = df_train['HasDetections']
del df_train['HasDetections']


chunk = len(df_train)//5 # 5 fold validation
idx = range(chunk*0,chunk//2) 
idx2 = range(chunk//2,chunk)
idx3 = range(chunk,chunk*3)
idx4 = range(chunk*3,chunk*5)
X_val1 = df_train.loc[idx]
Y_val1 = df_train_Y.loc[idx]
X_val2 = df_train.loc[idx2]
Y_val2 = df_train_Y.loc[idx2]
X_train1 = df_train.loc[idx3]
Y_train1 = df_train_Y.loc[idx3]
X_train2 = df_train.loc[idx4]
Y_train2 = df_train_Y.loc[idx4]



ins = []; outs = {}
# CREATE AN EMBEDDING FOR EACH CATEGORY VARIABLE
for k in inps.keys():
    x = Input(shape=(1,))
    ins.append(x)
    y = np.int(inps[k])
    x = Embedding(y, y, input_length=1)(x) 
    x = Reshape(target_shape=(y, ))(x)
    outs[k]=x 
    
# ORGANIZE EMBEDDINGS INTO GROUPS
all = set(inps.keys())
used = []
outs2 = []
for k in groups:
    g = [outs[x] for x in set(k).intersection(all)]
    used += list(set(k).intersection(all))
    x = concatenate(g)
    s = sum([inps[x] for x in set(k).intersection(all)])
    x = Dense(s//2,kernel_initializer='he_uniform')(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    outs2.append(x)
g = [outs[x] for x in all-set(used)]
x = concatenate(g)
s = sum([inps[x] for x in all-set(used)])
x = Dense(s//2,kernel_initializer='he_uniform')(x)
x = BatchNormalization()(x)
x = Activation('elu')(x)
outs2.append(x)

x = Input(shape=(len(NUM), ))
ins.append(x)
x = Dense(len(NUM)//2,kernel_initializer='he_uniform')(x)
x = BatchNormalization()(x)
x = Activation('elu')(x) 

x = concatenate(outs2+[x])
x = Dense(100,kernel_initializer='he_uniform')(x)
x = Dropout(0.2)(x)
x = BatchNormalization()(x)
x = Activation('elu')(x)
x = Dense(100,kernel_initializer='he_uniform')(x)
x = Dropout(0.2)(x)
x = BatchNormalization()(x)
x = Activation('elu')(x)
x = Dense(100,kernel_initializer='he_uniform')(x)
x = Dropout(0.2)(x)
x = BatchNormalization()(x)
x = Activation('elu')(x)
x = Dense(1,activation='sigmoid')(x)

model = Model(inputs=ins, outputs=x)
model.compile(optimizer=Adam(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])
#annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)

epochs=10
batch=256
for k in range(epochs):
    model.fit( [X_train1[col] for col in Onehot_Encoding] + [X_train1[NUM]],Y_train1,
        batch_size=batch, epochs = 1, verbose=2, callbacks=[ #annealer, 
        printAUC(X_train1, Y_train1, Onehot_Encoding, NUM, X_val1, Y_val1, 0, k)],
        validation_data = ([X_val1[col] for col in Onehot_Encoding] + [X_val1[NUM]],Y_val1) )
    model.fit( [X_train2[col] for col in Onehot_Encoding] + [X_train2[NUM]],Y_train2,
        batch_size=batch, epochs = 1, verbose=2, callbacks=[ #annealer, 
        printAUC(X_train2, Y_train2, Onehot_Encoding, NUM, X_val2, Y_val2, 0, k)],
        validation_data = ([X_val2[col] for col in Onehot_Encoding] + [X_val2[NUM]],Y_val2) )
    # SHUFFLE TRAIN
    X_train1['HasDetections'] = Y_train1
    X_train1 = X_train1.sample(frac=1)
    Y_train1 = X_train1['HasDetections']
    del X_train1['HasDetections']
    X_train2['HasDetections'] = Y_train2
    X_train2 = X_train2.sample(frac=1)
    Y_train2 = X_train2['HasDetections']
    del X_train2['HasDetections']

pred = np.zeros((1000000,1))
id = 1
chunksize = 200000

for df_test in pd.read_csv('../input/test.csv', 
            chunksize = chunksize, usecols=list(dtypes.keys())[0:-1], dtype=dtypes):
    cols = []
    for x in FE:
        cols += encode_FE(df_test,x,verbose=0)
    for x in range(len(OHE)):
        cols += encode_OHE_test(df_test,OHE[x],dd[x])
    end = (id)*chunksize
    if end>1000000: end = 1000000
    pred[(id-1)*chunksize:end] = model.predict(df_test[cols])
    print('  encoded and predicted part',id)
    id += 1